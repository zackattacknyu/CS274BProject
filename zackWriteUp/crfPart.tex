


\section{Model: Exponential Family with Conditional Random Field}

\subsection{Introduction}

Conditional Random Fields are discriminative models for the conditional distribution of labels $y$ given observations $x$. For our purposes it will be defined over a graph structure as follows:
\[
p(y|x) = \frac{1}{Z(x)} \prod_i{\psi(y_i,x)} \prod_{c}{\psi(y_c,x)}
\]
Here, $Z(x)$ is the partition function that normalizes the distribution, $\psi(y_i,x)$ is the potential function for the states of node $i$ given the input $x$, and $\psi(y_c,x)$ is the potential function for the states of nodes in clique $c$ given the input $x$. \\
\\
We will take a 2-D image and make each pixel a separate node in our graph. Our graph will be a 4-connected grid so two pixels will be connected via an edge if they are immediate neighbors horizontally or vertically. \\
\\
Our input consists of features for each node and edge, denoted by $x_i$ and $w_c$ respectively. The edge features for clique $c$ are used to obtain a conditional distribution of values for the nodes connected by $c$ and the node features for node $i$ are used to obtain a distribution of values for only node $i$. Our equation now becomes as follows:
\[
p(y|x) = \frac{1}{Z(x)} \prod_i{\psi(y_i,x_i)} \prod_{c}{\psi(y_c,w_c)}
\]
For our purposes, $\psi$ will be an exponential family. We will learn two matrices $\theta$ and $\phi$. The matrix $\theta$ will be multiplied by the node features to determine the log potentials for each value of node $i$. The matrix $\phi$ will be multiplied by the edge features to determine the log potentials for each pair of values for the nodes $i$ and $j$ connected by an edge in our graph. The model is now as follows:
\[
p(y|x) = \frac{1}{Z(x)} \prod_i{exp(\theta x_i)} \prod_{ij}{exp(\phi w_{ij})}
\]
Our goal will be to find $\theta$ and $\phi$ matrices that minimize a loss function given our training data. In our results, the loss function is defined as the clique logistic loss (Domke 2013)
\[
L = - \sum_c {log \, \mu(y_c; \theta, \phi, x_i, x_j, w_c)}
\]
Here $\mu$ signfies the accuracy of the clique marginal. For learning, we used the Back TRW algorithm (Domke 2013).

\subsection{Feature Setup}

The features associated with each node consist of the temperature as well as the 12 cloud patch features from the CCS data. There is one additional feature that is a constant. The cloud patch features are zero when there is no cloud. Additionally, we added an additional feature that is 1 for a cloud pixel and 0 for a non-cloud pixel to be sure that $\theta$ focuses only on precipitation versus no precipitation pixels for the cloud ones. There are thus a total of 15 features for each node. \\
\\
The edge features were reused from the vision application where the code came from. They consist of the absolute value of the temperature difference between neighboring pixels, parametrized as a set of 20 thresholds. There is also a constant. Additionally, a second set of features were added. The first set is only used by horizontal edges and the second set is only used by vertical edges. This way the edges are parameterized separately. 

\subsection{Results}

Our training data consisted of temperature and precipitation information from September 2011. There were readings every half hour thus there were a total of 1440 maps used in training. We then used maps from September 2012 as our test set. \\
\\
Here is an example of one of the September 2012 maps. These are the target precipiation labels. \\
**INSERT FIGURE OF TARGET LABELS**\\
Here are the marginal probabilities of rainfall across the image.\\
**INSERT FIGURE OF RAINFALL PROBS**\\
Because the marginal probabilities rarely exceeded $0.5$ we needed a smaller threshold for our predictor. We thus plotted the ROC curve for this map in order to see what a good score threshold would be:\\
**INSERT ROC CURVE** \\
If we use **INSERT VALUE** as our classification threshold then here is the map\\
**INSERT MAP**\\
Here are some of the statistics \\
**TABLE OF PROBABILITY INFORMATION FOR THE MAP**

\subsection{Conclusion}

There are problems with this model. There is low probability of precipitation even in the pixels where it occurred. This could be due to the fact that we have highly imbalanced classes. There could also be problems with our edge features as they are inspired by a different application. \\
\\
One potential fix is to use a loss function that weights incorrect guesses for precipitation pixels higher than guesses for other pixels. \\
\\
Another potential fix is a rethinking of the edge features. The current edge features do not correlate well **INSERT STATISTIC SHOWING THIS** with the target pairs of states for the nodes. We need to find edge features that show some correlation with pairs of states for neighboring nodes. One idea is to use the temperature difference but only for the cloud pixels. Another idea is to use one of the cloud patch features. We could also use a combination of mean temperature between pixels and difference between them. 


